{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4d41f348-2604-4780-8553-f609134190ca",
      "metadata": {
        "id": "4d41f348-2604-4780-8553-f609134190ca"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install torch peft==0.4.0 bitsandbytes==0.40.2  trl==0.4.7 accelerate einops tqdm scipy accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "706bf0bb-b269-44bc-95c0-492cf0c62cf8",
      "metadata": {
        "id": "706bf0bb-b269-44bc-95c0-492cf0c62cf8",
        "outputId": "139429fe-9a29-4d80-f133-6235033b3706",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'top_k_top_p_filtering' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-eccb719857a4>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFTTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.4.7\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBestOfNSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimport_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_peft_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtop_k_top_p_filtering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'top_k_top_p_filtering' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from datasets import load_from_disk\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from trl import SFTTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82db6546-2223-4dea-8b72-1419a1564eed",
      "metadata": {
        "id": "82db6546-2223-4dea-8b72-1419a1564eed"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"Amod/mental_health_counseling_conversations\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e741323e-e189-44cf-9586-a635283ea2e3",
      "metadata": {
        "id": "e741323e-e189-44cf-9586-a635283ea2e3"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69ea8e98-eb71-4528-9000-87db6d318140",
      "metadata": {
        "id": "69ea8e98-eb71-4528-9000-87db6d318140"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5580f060-de63-4f3f-bad2-d9633406b35b",
      "metadata": {
        "id": "5580f060-de63-4f3f-bad2-d9633406b35b"
      },
      "outputs": [],
      "source": [
        "# Function to transform the row into desired format\n",
        "def format_row(row):\n",
        "    question = row['Context']\n",
        "    answer = row['Response']\n",
        "    formatted_string = f\"[INST] {question} [/INST] {answer} \"\n",
        "    return formatted_string\n",
        "\n",
        "# Apply the function to each row of the dataframe\n",
        "df['Formatted'] = df.apply(format_row, axis=1)\n",
        "\n",
        "# Display the formatted column\n",
        "df['Formatted']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a3ea251-2c4b-4b95-91ba-ffbc4beb2ceb",
      "metadata": {
        "id": "4a3ea251-2c4b-4b95-91ba-ffbc4beb2ceb"
      },
      "outputs": [],
      "source": [
        "# Rename the 'Formatted' column to 'Text'\n",
        "new_df = df.rename(columns={'Formatted': 'Text'})\n",
        "\n",
        "new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60ee9dd9-0b60-4dab-9dbc-ee5848663bca",
      "metadata": {
        "id": "60ee9dd9-0b60-4dab-9dbc-ee5848663bca"
      },
      "outputs": [],
      "source": [
        "new_df = new_df[['Text']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "131ec38f-1e99-4f0b-8dc6-0a2c5158942a",
      "metadata": {
        "id": "131ec38f-1e99-4f0b-8dc6-0a2c5158942a"
      },
      "outputs": [],
      "source": [
        "new_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6df85c8f-d554-4988-b266-a97d1a0dc679",
      "metadata": {
        "id": "6df85c8f-d554-4988-b266-a97d1a0dc679"
      },
      "outputs": [],
      "source": [
        "# If you want to save the new dataframe to a CSV file:\n",
        "new_df.to_csv('formatted_data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "379dc118-1fe6-4a1e-9e97-86ac70443f61",
      "metadata": {
        "id": "379dc118-1fe6-4a1e-9e97-86ac70443f61"
      },
      "outputs": [],
      "source": [
        "final_df = pd.read_csv(\"formatted_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78f62718-8c64-4508-94ce-ec6832079fc6",
      "metadata": {
        "id": "78f62718-8c64-4508-94ce-ec6832079fc6"
      },
      "outputs": [],
      "source": [
        "final_df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b86e1450-5d7b-43c0-b6c0-7bbbf4c0b667",
      "metadata": {
        "id": "b86e1450-5d7b-43c0-b6c0-7bbbf4c0b667"
      },
      "outputs": [],
      "source": [
        "training_dataset = load_dataset(\"csv\", data_files=\"formatted_data.csv\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "408d12aa-bba2-4ab5-a72f-b3d87d7c9298",
      "metadata": {
        "id": "408d12aa-bba2-4ab5-a72f-b3d87d7c9298"
      },
      "outputs": [],
      "source": [
        "training_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1499c42d-34c7-424b-aead-5b42f19e2f7e",
      "metadata": {
        "id": "1499c42d-34c7-424b-aead-5b42f19e2f7e"
      },
      "outputs": [],
      "source": [
        "base_model = \"microsoft/phi-2\"\n",
        "new_model = \"phi-2-mental-health\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
        "tokenizer.pad_token=tokenizer.eos_token\n",
        "tokenizer.padding_side=\"right\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "rd3jc7LX9HsM"
      },
      "id": "rd3jc7LX9HsM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip uninstall -y transformers\n",
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "metadata": {
        "id": "4nDZ7uQUA4wR"
      },
      "id": "4nDZ7uQUA4wR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    #quantization_config=bnb_config,\n",
        "    # use_flash_attention_2=True, # Phi does not support yet.\n",
        "    trust_remote_code=True,\n",
        "    #flash_attn=True,\n",
        "    #flash_rotary=True,\n",
        "    #fused_dense=True,\n",
        "    low_cpu_mem_usage=True\n",
        "    #device_map=\"cpu\",\n",
        "    #revision=\"refs/pr/23\",\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "m_PeCIV99JXn"
      },
      "id": "m_PeCIV99JXn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "khCZx0iz9Rwi"
      },
      "id": "khCZx0iz9Rwi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=32,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    logging_steps=15,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    learning_rate=2e-4,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    save_steps=200,\n",
        "    warmup_ratio=0.05,\n",
        "    weight_decay=0.01,\n",
        "    max_steps=-1\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "3Hxo84HK6VU9"
      },
      "id": "3Hxo84HK6VU9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules= [\"Wqkv\", \"fc1\", \"fc2\" ] # [\"Wqkv\", \"out_proj\", \"fc1\", \"fc2\" ], - 41M params\n",
        "    # modules_to_save=[\"embed_tokens\",\"lm_head\"]\n",
        ")\n"
      ],
      "metadata": {
        "id": "owlg88Po6XTX"
      },
      "id": "owlg88Po6XTX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=training_dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"Text\",\n",
        "    max_seq_length=690,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")"
      ],
      "metadata": {
        "id": "KMhPj04r6YkL"
      },
      "id": "KMhPj04r6YkL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b14dc5cc-202f-4c0a-9b7b-de3be2561c50",
      "metadata": {
        "id": "b14dc5cc-202f-4c0a-9b7b-de3be2561c50",
        "outputId": "fb11775b-46b9-4c91-8228-7d19bcbb5e20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a CodeGenTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b83523b5-bf38-4e6f-aa48-9bedc3423ccf",
      "metadata": {
        "id": "b83523b5-bf38-4e6f-aa48-9bedc3423ccf"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f1ef8f8-df5b-4776-ac09-0ff2fc408a5c",
      "metadata": {
        "id": "2f1ef8f8-df5b-4776-ac09-0ff2fc408a5c"
      },
      "outputs": [],
      "source": [
        "# Run text generation pipeline with our next model\n",
        "prompt = \"I am not able to sleep in night. Do you have any suggestions?\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=250)\n",
        "result = pipe(f\"[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8845396-23eb-47fd-be5e-cb4a13086607",
      "metadata": {
        "id": "f8845396-23eb-47fd-be5e-cb4a13086607"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "del pipe\n",
        "del trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d55b15c4-0ce8-45a9-bf31-b025cf2ef37c",
      "metadata": {
        "id": "d55b15c4-0ce8-45a9-bf31-b025cf2ef37c"
      },
      "outputs": [],
      "source": [
        "model_name = \"microsoft/phi-2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "809076bc-99d9-4d6a-b6e9-26cec0d0b134",
      "metadata": {
        "id": "809076bc-99d9-4d6a-b6e9-26cec0d0b134"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecc7034a-734d-46bf-a084-779cd15475cf",
      "metadata": {
        "id": "ecc7034a-734d-46bf-a084-779cd15475cf"
      },
      "outputs": [],
      "source": [
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/phi-2\",\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "155dde0f-63f6-42a3-8fb2-2f540d5762af",
      "metadata": {
        "id": "155dde0f-63f6-42a3-8fb2-2f540d5762af"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b37d7f7-e0d2-42a2-b550-bd5e10b69a1d",
      "metadata": {
        "id": "4b37d7f7-e0d2-42a2-b550-bd5e10b69a1d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}