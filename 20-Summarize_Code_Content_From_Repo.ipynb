{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "15Ob4cllau3CbcyyVjWcPwYbewtCg9M6-",
      "authorship_tag": "ABX9TyN9QLzQNnJozhQ/59s+c51x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehdi-lamrani/LLM/blob/main/20-Summarize_Code_Content_From_Repo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_E8hOcn5Fk5",
        "outputId": "775646e5-5fab-4e49-b94f-b91088a14f05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEJRvQzK5ETs"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL of the GitHub page\n",
        "url = \"https://github.com/mehdi-lamrani/LLM/tree/main\"  # Replace with the actual GitHub page URL\n",
        "\n",
        "# Fetch the page\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all <a> tags\n",
        "    links = soup.find_all('a')\n",
        "\n",
        "    # Extract the href attributes that end with .ipynb\n",
        "    ipynb_links = [link.get('href') for link in links if link.get('href') and link.get('href').endswith('.ipynb')]\n",
        "    ipynb_raw=[]\n",
        "    # Print all .ipynb links\n",
        "    for href in ipynb_links:\n",
        "        ipynb_raw.append(\"https://raw.githubusercontent.com\"+href.replace(\"blob/\",\"\"))\n",
        "else:\n",
        "    print(\"Failed to retrieve the page\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nbconvert"
      ],
      "metadata": {
        "id": "28PvcpB76BAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipynb-py-convert\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJgrSq8193nH",
        "outputId": "d6ce7a75-565a-499d-e0ff-be641a81d678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipynb-py-convert\n",
            "  Downloading ipynb-py-convert-0.4.6.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ipynb-py-convert\n",
            "  Building wheel for ipynb-py-convert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipynb-py-convert: filename=ipynb_py_convert-0.4.6-py3-none-any.whl size=4623 sha256=1992940c5f2f8004f096dec1c89bf875a3d1b3bd372c3b169fb2c6143a3beaea\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/a2/b7/2816fda86a647adbe8c4e7b7f4cc72cdc37720ad11b1611af5\n",
            "Successfully built ipynb-py-convert\n",
            "Installing collected packages: ipynb-py-convert\n",
            "Successfully installed ipynb-py-convert-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "for url in ipynb_raw:\n",
        "    with open(\"nb/\"+url.split('/')[-1], 'wb') as f:\n",
        "        f.write(requests.get(url).content)"
      ],
      "metadata": {
        "id": "2LKzzega7RMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to script nb/*.ipynb"
      ],
      "metadata": {
        "id": "OqIupA9S9iaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!for file in nb/*.txt; do mv \"$file\" \"${file%.txt}.py\"; done\n"
      ],
      "metadata": {
        "id": "tfEMoP2P_pPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!for file in nb/*.py; do mv \"$file\" py/; done\n"
      ],
      "metadata": {
        "id": "Y9V2rm8N_-Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install langchain langchain_openai"
      ],
      "metadata": {
        "id": "dQTlnt3ZCp05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "llm=OpenAI()\n",
        "llm(\"Say Hello\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "9tuii5nRCTVL",
        "outputId": "423de5fb-31a7-498f-8939-b53599f8ddc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" to\\n\\nthe Future of\\n\\nStock Trading\\n\\nThe world's first social stock trading platform\\n\\nTradeWallStreet allows you to interact with traders from all over the world, build trading models, and share insights all in one place. You can trade stocks and options with algorithms and collaborate in the community to build better models and earn rewards.\\n\\nGet early access\\n\\nTradeWallStreet is a new way to trade\\n\\nWe are changing the way people trade by allowing the community to come together to share ideas, collaborate, and build better trading algorithms.\\n\\nTrade stocks and options with your own algorithms, create groups, and connect with traders from all over the world.\\n\\nEarn rewards by helping others\\n\\nEarn rewards and win prizes by helping others in the community. Share your insights, collaborate, and build better trading models together.\\n\\nTradeWallStreet is also the first platform to offer a rewards program for successful trades made by members. Your contributions to the community will be recognized and rewarded.\\n\\nConnect with traders from all over the world\\n\\nTradeWallStreet is a global community of traders from all over the world. Connect with like-minded individuals, learn from each other, and collaborate to build better trading models.\\n\\nOur platform allows you to easily connect with traders, join groups, and share insights. No matter where you\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Open the file\n",
        "with open('/content/drive/MyDrive/py/01-DB_FT_HF_PEFT_LORA_SAVE.py', 'r') as file:\n",
        "    # Read the first line\n",
        "    code = file.read()\n",
        "code\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "CAXvhN2MAqC6",
        "outputId": "d8b371eb-e6ee-4463-a46e-cf43360a455c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#!/usr/bin/env python\\n# coding: utf-8\\n\\n# # Fine-tuning Sandbox\\n# \\n# Code authored by: Shawhin Talebi <br>\\n# Blog link: https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91\\n\\n# In[ ]:\\n\\n\\nfrom datasets import load_dataset, DatasetDict, Dataset\\n\\nfrom transformers import (\\n    AutoTokenizer,\\n    AutoConfig,\\n    AutoModelForSequenceClassification,\\n    DataCollatorWithPadding,\\n    TrainingArguments,\\n    Trainer)\\n\\nfrom peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\\nimport evaluate\\nimport torch\\nimport numpy as np\\n\\n\\n# ### dataset\\n\\n# In[ ]:\\n\\n\\n# # how dataset was generated\\n\\n# # load imdb data\\n# imdb_dataset = load_dataset(\"imdb\")\\n\\n# # define subsample size\\n# N = 1000\\n# # generate indexes for random subsample\\n# rand_idx = np.random.randint(24999, size=N)\\n\\n# # extract train and test data\\n# x_train = imdb_dataset[\\'train\\'][rand_idx][\\'text\\']\\n# y_train = imdb_dataset[\\'train\\'][rand_idx][\\'label\\']\\n\\n# x_test = imdb_dataset[\\'test\\'][rand_idx][\\'text\\']\\n# y_test = imdb_dataset[\\'test\\'][rand_idx][\\'label\\']\\n\\n# # create new dataset\\n# dataset = DatasetDict({\\'train\\':Dataset.from_dict({\\'label\\':y_train,\\'text\\':x_train}),\\n#                              \\'validation\\':Dataset.from_dict({\\'label\\':y_test,\\'text\\':x_test})})\\n\\n\\n# In[ ]:\\n\\n\\n# load dataset\\ndataset = load_dataset(\\'shawhin/imdb-truncated\\')\\ndataset\\n\\n\\n# In[ ]:\\n\\n\\n# display % of training data with label=1\\nnp.array(dataset[\\'train\\'][\\'label\\']).sum()/len(dataset[\\'train\\'][\\'label\\'])\\n\\n\\n# ### model\\n\\n# In[ ]:\\n\\n\\nmodel_checkpoint = \\'distilbert-base-uncased\\'\\n# model_checkpoint = \\'roberta-base\\' # you can alternatively use roberta-base but this model is bigger thus training will take longer\\n\\n# define label maps\\nid2label = {0: \"Negative\", 1: \"Positive\"}\\nlabel2id = {\"Negative\":0, \"Positive\":1}\\n\\n# generate classification model from model_checkpoint\\nmodel = AutoModelForSequenceClassification.from_pretrained(\\n    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)\\n\\n\\n# In[ ]:\\n\\n\\n# display architecture\\nmodel\\n\\n\\n# ### preprocess data\\n\\n# In[ ]:\\n\\n\\n# create tokenizer\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\\n\\n# add pad token if none exists\\nif tokenizer.pad_token is None:\\n    tokenizer.add_special_tokens({\\'pad_token\\': \\'[PAD]\\'})\\n    model.resize_token_embeddings(len(tokenizer))\\n\\n\\n# In[ ]:\\n\\n\\n# create tokenize function\\ndef tokenize_function(examples):\\n    # extract text\\n    text = examples[\"text\"]\\n\\n    #tokenize and truncate text\\n    tokenizer.truncation_side = \"left\"\\n    tokenized_inputs = tokenizer(\\n        text,\\n        return_tensors=\"np\",\\n        truncation=True,\\n        max_length=512\\n    )\\n\\n    return tokenized_inputs\\n\\n\\n# In[ ]:\\n\\n\\n# tokenize training and validation datasets\\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\\ntokenized_dataset\\n\\n\\n# In[ ]:\\n\\n\\n# create data collator\\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\\n\\n\\n# ### evaluation\\n\\n# In[ ]:\\n\\n\\n# import accuracy evaluation metric\\naccuracy = evaluate.load(\"accuracy\")\\n\\n\\n# In[ ]:\\n\\n\\n# define an evaluation function to pass into trainer later\\ndef compute_metrics(p):\\n    predictions, labels = p\\n    predictions = np.argmax(predictions, axis=1)\\n\\n    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}\\n\\n\\n# ### Apply untrained model to text\\n\\n# In[ ]:\\n\\n\\n# define list of examples\\ntext_list = [\"It was good.\", \"Not a fan, don\\'t recommed.\", \"Better than the first one.\", \"This is not worth watching even once.\", \"This one is a pass.\"]\\n\\nprint(\"Untrained model predictions:\")\\nprint(\"----------------------------\")\\nfor text in text_list:\\n    # tokenize text\\n    inputs = tokenizer.encode(text, return_tensors=\"pt\")\\n    # compute logits\\n    logits = model(inputs).logits\\n    # convert logits to label\\n    predictions = torch.argmax(logits)\\n\\n    print(text + \" - \" + id2label[predictions.tolist()])\\n\\n\\n# ### Train model\\n\\n# In[ ]:\\n\\n\\npeft_config = LoraConfig(task_type=\"SEQ_CLS\",\\n                        r=4,\\n                        lora_alpha=32,\\n                        lora_dropout=0.01,\\n                        target_modules = [\\'q_lin\\'])\\n\\n\\n# In[ ]:\\n\\n\\npeft_config\\n\\n\\n# In[ ]:\\n\\n\\nmodel = get_peft_model(model, peft_config)\\nmodel.print_trainable_parameters()\\n\\n\\n# In[ ]:\\n\\n\\n# hyperparameters\\nlr = 1e-3\\nbatch_size = 4\\nnum_epochs = 10\\n\\n\\n# In[ ]:\\n\\n\\n# define training arguments\\ntraining_args = TrainingArguments(\\n    output_dir= model_checkpoint + \"-lora-text-classification\",\\n    learning_rate=lr,\\n    per_device_train_batch_size=batch_size,\\n    per_device_eval_batch_size=batch_size,\\n    num_train_epochs=num_epochs,\\n    weight_decay=0.01,\\n    evaluation_strategy=\"epoch\",\\n    save_strategy=\"epoch\",\\n    load_best_model_at_end=True,\\n)\\n\\n\\n# In[ ]:\\n\\n\\n# creater trainer object\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=tokenized_dataset[\"train\"],\\n    eval_dataset=tokenized_dataset[\"validation\"],\\n    tokenizer=tokenizer,\\n    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\\n    compute_metrics=compute_metrics,\\n)\\n\\n# train model\\ntrainer.train()\\n\\n\\n# ### Generate prediction\\n\\n# In[ ]:\\n\\n\\nmodel.to(\\'mps\\') # moving to mps for Mac (can alternatively do \\'cpu\\')\\n\\nprint(\"Trained model predictions:\")\\nprint(\"--------------------------\")\\nfor text in text_list:\\n    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"mps\") # moving to mps for Mac (can alternatively do \\'cpu\\')\\n\\n    logits = model(inputs).logits\\n    predictions = torch.max(logits,1).indices\\n\\n    print(text + \" - \" + id2label[predictions.tolist()[0]])\\n\\n\\n# ### Optional: push model to hub\\n\\n# In[ ]:\\n\\n\\n# option 1: notebook login\\nfrom huggingface_hub import notebook_login\\nnotebook_login() # ensure token gives write access\\n\\n# # option 2: key login\\n# from huggingface_hub import login\\n# write_key = \\'hf_\\' # paste token here\\n# login(write_key)\\n\\n\\n# In[ ]:\\n\\n\\nhf_name = \\'shawhin\\' # your hf username or org name\\nmodel_id = hf_name + \"/\" + model_checkpoint + \"-lora-text-classification\" # you can name the model whatever you want\\n\\n\\n# In[ ]:\\n\\n\\nmodel.push_to_hub(model_id) # save model\\n\\n\\n# In[ ]:\\n\\n\\ntrainer.push_to_hub(model_id) # save trainer\\n\\n\\n# ### Optional: load peft model\\n\\n# In[ ]:\\n\\n\\n# how to load peft model from hub for inference\\nconfig = PeftConfig.from_pretrained(model_id)\\ninference_model = AutoModelForSequenceClassification.from_pretrained(\\n    config.base_model_name_or_path, num_labels=2, id2label=id2label, label2id=label2id\\n)\\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\\nmodel = PeftModel.from_pretrained(inference_model, model_id)\\n\\n\\n# In[ ]:\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "BInKXDFsE97Y",
        "outputId": "dc3536bd-3803-4f8f-a7e9-a96b48fd08d5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#!/usr/bin/env python\\n# coding: utf-8\\n\\n# # Fine-tuning Sandbox\\n# \\n# Code authored by: Shawhin Talebi <br>\\n# Blog link: https://medium.com/towards-data-science/fine-tuning-large-language-models-llms-23473d763b91\\n\\n# In[ ]:\\n\\n\\nfrom datasets import load_dataset, DatasetDict, Dataset\\n\\nfrom transformers import (\\n    AutoTokenizer,\\n    AutoConfig,\\n    AutoModelForSequenceClassification,\\n    DataCollatorWithPadding,\\n    TrainingArguments,\\n    Trainer)\\n\\nfrom peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\\nimport evaluate\\nimport torch\\nimport numpy as np\\n\\n\\n# ### dataset\\n\\n# In[ ]:\\n\\n\\n# # how dataset was generated\\n\\n# # load imdb data\\n# imdb_dataset = load_dataset(\"imdb\")\\n\\n# # define subsample size\\n# N = 1000\\n# # generate indexes for random subsample\\n# rand_idx = np.random.randint(24999, size=N)\\n\\n# # extract train and test data\\n# x_train = imdb_dataset[\\'train\\'][rand_idx][\\'text\\']\\n# y_train = imdb_dataset[\\'train\\'][rand_idx][\\'label\\']\\n\\n# x_test = imdb_dataset[\\'test\\'][rand_idx][\\'text\\']\\n# y_test = imdb_dataset[\\'test\\'][rand_idx][\\'label\\']\\n\\n# # create new dataset\\n# dataset = DatasetDict({\\'train\\':Dataset.from_dict({\\'label\\':y_train,\\'text\\':x_train}),\\n#                              \\'validation\\':Dataset.from_dict({\\'label\\':y_test,\\'text\\':x_test})})\\n\\n\\n# In[ ]:\\n\\n\\n# load dataset\\ndataset = load_dataset(\\'shawhin/imdb-truncated\\')\\ndataset\\n\\n\\n# In[ ]:\\n\\n\\n# display % of training data with label=1\\nnp.array(dataset[\\'train\\'][\\'label\\']).sum()/len(dataset[\\'train\\'][\\'label\\'])\\n\\n\\n# ### model\\n\\n# In[ ]:\\n\\n\\nmodel_checkpoint = \\'distilbert-base-uncased\\'\\n# model_checkpoint = \\'roberta-base\\' # you can alternatively use roberta-base but this model is bigger thus training will take longer\\n\\n# define label maps\\nid2label = {0: \"Negative\", 1: \"Positive\"}\\nlabel2id = {\"Negative\":0, \"Positive\":1}\\n\\n# generate classification model from model_checkpoint\\nmodel = AutoModelForSequenceClassification.from_pretrained(\\n    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)\\n\\n\\n# In[ ]:\\n\\n\\n# display architecture\\nmodel\\n\\n\\n# ### preprocess data\\n\\n# In[ ]:\\n\\n\\n# create tokenizer\\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\\n\\n# add pad token if none exists\\nif tokenizer.pad_token is None:\\n    tokenizer.add_special_tokens({\\'pad_token\\': \\'[PAD]\\'})\\n    model.resize_token_embeddings(len(tokenizer))\\n\\n\\n# In[ ]:\\n\\n\\n# create tokenize function\\ndef tokenize_function(examples):\\n    # extract text\\n    text = examples[\"text\"]\\n\\n    #tokenize and truncate text\\n    tokenizer.truncation_side = \"left\"\\n    tokenized_inputs = tokenizer(\\n        text,\\n        return_tensors=\"np\",\\n        truncation=True,\\n        max_length=512\\n    )\\n\\n    return tokenized_inputs\\n\\n\\n# In[ ]:\\n\\n\\n# tokenize training and validation datasets\\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\\ntokenized_dataset\\n\\n\\n# In[ ]:\\n\\n\\n# create data collator\\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\\n\\n\\n# ### evaluation\\n\\n# In[ ]:\\n\\n\\n# import accuracy evaluation metric\\naccuracy = evaluate.load(\"accuracy\")\\n\\n\\n# In[ ]:\\n\\n\\n# define an evaluation function to pass into trainer later\\ndef compute_metrics(p):\\n    predictions, labels = p\\n    predictions = np.argmax(predictions, axis=1)\\n\\n    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}\\n\\n\\n# ### Apply untrained model to text\\n\\n# In[ ]:\\n\\n\\n# define list of examples\\ntext_list = [\"It was good.\", \"Not a fan, don\\'t recommed.\", \"Better than the first one.\", \"This is not worth watching even once.\", \"This one is a pass.\"]\\n\\nprint(\"Untrained model predictions:\")\\nprint(\"----------------------------\")\\nfor text in text_list:\\n    # tokenize text\\n    inputs = tokenizer.encode(text, return_tensors=\"pt\")\\n    # compute logits\\n    logits = model(inputs).logits\\n    # convert logits to label\\n    predictions = torch.argmax(logits)\\n\\n    print(text + \" - \" + id2label[predictions.tolist()])\\n\\n\\n# ### Train model\\n\\n# In[ ]:\\n\\n\\npeft_config = LoraConfig(task_type=\"SEQ_CLS\",\\n                        r=4,\\n                        lora_alpha=32,\\n                        lora_dropout=0.01,\\n                        target_modules = [\\'q_lin\\'])\\n\\n\\n# In[ ]:\\n\\n\\npeft_config\\n\\n\\n# In[ ]:\\n\\n\\nmodel = get_peft_model(model, peft_config)\\nmodel.print_trainable_parameters()\\n\\n\\n# In[ ]:\\n\\n\\n# hyperparameters\\nlr = 1e-3\\nbatch_size = 4\\nnum_epochs = 10\\n\\n\\n# In[ ]:\\n\\n\\n# define training arguments\\ntraining_args = TrainingArguments(\\n    output_dir= model_checkpoint + \"-lora-text-classification\",\\n    learning_rate=lr,\\n    per_device_train_batch_size=batch_size,\\n    per_device_eval_batch_size=batch_size,\\n    num_train_epochs=num_epochs,\\n    weight_decay=0.01,\\n    evaluation_strategy=\"epoch\",\\n    save_strategy=\"epoch\",\\n    load_best_model_at_end=True,\\n)\\n\\n\\n# In[ ]:\\n\\n\\n# creater trainer object\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=tokenized_dataset[\"train\"],\\n    eval_dataset=tokenized_dataset[\"validation\"],\\n    tokenizer=tokenizer,\\n    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\\n    compute_metrics=compute_metrics,\\n)\\n\\n# train model\\ntrainer.train()\\n\\n\\n# ### Generate prediction\\n\\n# In[ ]:\\n\\n\\nmodel.to(\\'mps\\') # moving to mps for Mac (can alternatively do \\'cpu\\')\\n\\nprint(\"Trained model predictions:\")\\nprint(\"--------------------------\")\\nfor text in text_list:\\n    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"mps\") # moving to mps for Mac (can alternatively do \\'cpu\\')\\n\\n    logits = model(inputs).logits\\n    predictions = torch.max(logits,1).indices\\n\\n    print(text + \" - \" + id2label[predictions.tolist()[0]])\\n\\n\\n# ### Optional: push model to hub\\n\\n# In[ ]:\\n\\n\\n# option 1: notebook login\\nfrom huggingface_hub import notebook_login\\nnotebook_login() # ensure token gives write access\\n\\n# # option 2: key login\\n# from huggingface_hub import login\\n# write_key = \\'hf_\\' # paste token here\\n# login(write_key)\\n\\n\\n# In[ ]:\\n\\n\\nhf_name = \\'shawhin\\' # your hf username or org name\\nmodel_id = hf_name + \"/\" + model_checkpoint + \"-lora-text-classification\" # you can name the model whatever you want\\n\\n\\n# In[ ]:\\n\\n\\nmodel.push_to_hub(model_id) # save model\\n\\n\\n# In[ ]:\\n\\n\\ntrainer.push_to_hub(model_id) # save trainer\\n\\n\\n# ### Optional: load peft model\\n\\n# In[ ]:\\n\\n\\n# how to load peft model from hub for inference\\nconfig = PeftConfig.from_pretrained(model_id)\\ninference_model = AutoModelForSequenceClassification.from_pretrained(\\n    config.base_model_name_or_path, num_labels=2, id2label=id2label, label2id=label2id\\n)\\ntokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\\nmodel = PeftModel.from_pretrained(inference_model, model_id)\\n\\n\\n# In[ ]:\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = llm(f\"Summarize the following Code using bullet points  {code}\")\n",
        "print(out)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Eu0W4NKDGol",
        "outputId": "acdce60a-92ba-45fe-c10a-0b5a4ebc7dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "PTJ0iKXYFCrz",
        "outputId": "e5768fa5-47a6-439d-a2f1-bc8a594c41c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm=OpenAI()\n",
        "#instruction = \"Hello World\"\n",
        "instruction = \"7*7\"\n",
        "generated_py_code = llm(f\"Generate Python Code that prints {instruction}\")\n",
        "generated_py_code"
      ],
      "metadata": {
        "id": "X0Z9mGhqCKD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "llm = OpenAI()\n",
        "template = \"\"\"You will be provided python code. Summarize it in the following way :\n",
        "First, it is mandatory that you Insert bold titles for each relevant section,\n",
        "then use bullets point explaining in great detail what the code does.\n",
        "Question: {text}\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate(input_variables=[\"text\"], template=template)\n",
        "answer_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "answer = answer_chain.run(f\"Summarize the following Code using bullet points  {code}\")\n",
        "print(answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEDgGf6qI_tW",
        "outputId": "a9304d13-5fff-47a5-c6b4-beff038c90b6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- **Introduction**\n",
            "    - The code is a fine-tuning sandbox for large language models (LLMs). \n",
            "    - It is authored by Shawhin Talebi and the blog link is provided. \n",
            "- **Dataset**\n",
            "    - The dataset is generated by loading IMDB data. \n",
            "    - A subsample size of 1000 is defined. \n",
            "    - Indexes for a random subsample are generated. \n",
            "    - Train and test data are extracted. \n",
            "    - A new dataset is created with training and validation data. \n",
            "    - The dataset is then loaded. \n",
            "    - The percentage of training data with label=1 is displayed. \n",
            "- **Model**\n",
            "    - The model checkpoint is defined as distilbert-base-uncased. \n",
            "    - Label maps are defined with corresponding IDs. \n",
            "    - A classification model is generated from the model checkpoint. \n",
            "- **Preprocess Data**\n",
            "    - A tokenizer is created. \n",
            "    - A pad token is added if one is not present. \n",
            "    - A tokenize function is created. \n",
            "    - The datasets are then tokenized using the tokenize function. \n",
            "    - A data collator is created. \n",
            "- **Evaluation**\n",
            "    - The accuracy evaluation metric is imported. \n",
            "    - An\n"
          ]
        }
      ]
    }
  ]
}